# Stage config for running CosyVoice3 with 2 stage architecture
# Stage 0: Talker (text prompt → speech tokens)
# Stage 1: Code2Wav (flow matching → acoustic features → waveform)
# Right now I have coded up stage 2 in stage 1 only; can be split in future

stage_args:
  - stage_id: 0
    runtime:
      devices: 0
    engine_args:
      model_stage: talker
      worker_cls: vllm_omni.worker.gpu_ar_worker.GPUARWorker
      scheduler_cls: vllm_omni.core.sched.omni_ar_scheduler.OmniARScheduler
      model_arch: CosyVoice3Model
      trust_remote_code: true
      gpu_memory_utilization: 0.4
      engine_output_type: latent  # Output speech tokens for chunk aware flow matching
      disable_hybrid_kv_cache_manager: true
      enable_prefix_caching: false
      enforce_eager: false
      mm_processor_cache_gb: 0
      skip_mm_profiling: true
      dtype: "float32"

  - stage_id: 1
    runtime:
      devices: 0
    engine_args:
      model_stage: code2wav
      model_arch: CosyVoice3Model
      trust_remote_code: true
      worker_cls: vllm_omni.worker.gpu_generation_worker.GPUGenerationWorker
      scheduler_cls: vllm_omni.core.sched.omni_generation_scheduler.OmniGenerationScheduler
      engine_output_type: latent
      gpu_memory_utilization: 0.2
      enforce_eager: false  # CUDA graphs don't work with dynamic runtime_info access
      disable_hybrid_kv_cache_manager: true
      enable_prefix_caching: false
      skip_mm_profiling: true
      dtype: "float32"
    engine_input_source: [0]
    custom_process_input_func: vllm_omni.model_executor.stage_input_processors.cosyvoice3.text2flow
    final_output: true
    final_output_type: audio
